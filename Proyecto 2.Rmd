---
title: "Proyecto 2"
output: github_document
authors: Alejandro Manhood, Estivaliz Rosalez
---

# Proyecto 2

Para este proyecto se debe crear una playlist, la cuál debe tener una duración de 3 horas. La playlist que se creara estará orientada para el momento en que uno esta haciendo un viaje largo en la carretera. Por ende, esta playlist será variada, para poder ayudar en la motivación, pasar un buen rato y también tener la oportunidad de pensar.

Para esto se generarán clusters con las canciones de la base de datos; es importante señalar que se trabajará con una muestra de 10.000 observaciones, esto es debido a que al ser tan grande la base original, existen complicaciones para algunos computadores al momento de realizar los diferentes análisis que son necesarios. Es necesario recalcar que como se trabajará con una muestra de la base original se debe verificar que esta sea representativa de la orginal para esto se calcularan las medias y desviaciones, para buscar que las variaciones sean muy pequeñas. De esta manera se tendrá una muestra representativa de los datos.

Es importante mencionar que se realizará también una limpieza de la data. Esto con elobjetivo de poder retirar tanto elementos que esten repetidos, como canciones, y también retirar elementos que no hayan sido consignados. Así podremos reducir el tamaño de nuestra base de datos, para el momento de generar la muestra. 

## Carga de Librerías

Se añaden a nuestro código las librerías a utilizar para los análisis que realizaremos.

```{r}
library(tidyverse)
library(cluster)
library(factoextra)
library(janitor)
library(mclust)
library(utils)
```

## Cargar Datos Actividad

Dado el formato con el que estan subidos los datos, para cargar los datos se emplea la función load(), y de esta manera tenemos ya listos neustros datos para comenzar con los análisis pertinentes de esta tarea.

```{r}
load("beats.RData")
```

## Preprocesamiento de los datos

### Selección de variables de interés

Una vez cargados los datos, es necesario realizar una limpieza de estos, para así poder realizar el posterior análisis de mejor manera y sin errores en los procesos de clustering debido a datos faltantes por ejemplo. Y también para poder reducir el tamaño de la base de datos, ya que se necesita un computador con una gran capacidad de procesamiento para trabajar con aproximadamente 500.000 datos.

Así, lo primero corresponde a escoger solo aquellas columnas que sean relevantes para el caso de estudio, las que en este caso son aquellas que nos entregan información respecto de parametros de las canciones, desde que tan energéticas son, hasta aquelas que si se encuentran siendo grabadas con público, de esta manera podremos generar clusters y lograr crear la playlist que es de nuestro interés, que basicamente son todas las variables que nos entregan información respecto de las canciones, según la API de spotify, que el profesor nos dió para guiarnos.
        Danceability
        Energy
        Key
        Loudness    
        Mode
        Speechiness
        Acousticness
        Instrumentalness
        Liveness
        Valence
        Tempo

Además, también se deben conservar las columnas "track_id" para poder identificar la canción en spotify, "duration_ms" para luego determinar el largo de la playlists a crear y que de esta manera cumpla con el requisito de duración, el cual es de 3 horas. Finalmente "track_name" y "artist_name", para poder entregar una playlist que tenga sentido, ya que es de esperar que la persona que escuche la playlist desee poder además de saber el nombre de la canción, quiera conocer al artista para quizás explorar más respecto de este. Estas columnas serán guardadas en una nueva tabla que tendra por nombre mus1, luego se muestra un resumen de este y se muestra el número de filas. Percatandonos que tenemos 447.622 observaciones.

```{r}
mus1 <- beats[, c(1,8:19, 23, 27)]
summary(mus1)
nrow(mus1)
```

### Limpieza de datos NA

Al hacer el resumen de nuestra tabla filtrada, podemos ver que no existen datos que sean NA en las variables seleccionadas. Por ende se prosigue con el analisis para eliminar los elementos que se encuentren duplicados.

### Limpieza de datos duplicados

En este punto se procede a realizar la eliminación de observaciones que se encuentren repetidas. Podríamos pensar que con eliminar segun nombre de la canción nos encontraríamos listos, pero esto sería un error, ya que pueden existir canciones que tengan el mismo nombre pero sean de artista diferente. Por ende, la mejor opción es eliminar los "track_id" que se encuentren duplicados, ya que como estos son únicos si existe otra canción que tenga el mismo códgio, quiere decir que se encuentra duplicada la canción. Para esto se utiliza un chunk que aprendimos en una ayudantía. Adicionalmente se revisa la cantidad de filas para comparar cuantas se eliminaron luego del filtrado de duplicados.

Las canciones que son únicas se dejarán en una variable nueva que recibe el nombre de mus_uni.

```{r}
mus_uni <- mus1[!duplicated(mus1$track_id),]

nrow(mus1) - nrow(mus_uni)
```

Esto nos da como resultado que fueron eliminadas 2.525 canciones que se encontraban duplicadas. Luego se procede a revisar si es que no existen algun dato que se encuentre duplicado. Y aquí podemos corroborar que no tenemos datos NA. Lo cual es bueno porque no nos causará problemas en los próximos análisis.

```{r}
summary(mus_uni)
```

## Conversión de tipo de datos

Podemos ver en el resumen de mus_uni, que existen diversos tipos de datos. Por este motivo procederemos a unficarlos tanto en double (para los que son numéricos) y character para aquellos que sean palabras, de esta manera no tendremos inconvenientes por el tipo de dato al realizar los análisis. Primero se procedera a convertir las variables numericas a character para luego pasarlas a double, de esta manera evitaremos cualquier inconveniente que pudiese surgir producto de esto. (Según lo que se nos explico en una ayudantía)

```{r}
mus_uni$danceability <- as.double(as.character(mus_uni$danceability))
mus_uni$energy <- as.double(as.character(mus_uni$energy))
mus_uni$key <- as.double(as.character(mus_uni$key))
mus_uni$loudness <- as.double(as.character(mus_uni$loudness))
mus_uni$mode <- as.double(as.character(mus_uni$mode))
mus_uni$speechiness <- as.double(as.character(mus_uni$speechiness)) 
mus_uni$acousticness <- as.double(as.character(mus_uni$acousticness))
mus_uni$instrumentalness <- as.double(as.character(mus_uni$instrumentalness))
mus_uni$liveness <- as.double(as.character(mus_uni$liveness))
mus_uni$valence <- as.double(as.character(mus_uni$valence))
mus_uni$tempo <- as.double(as.character(mus_uni$tempo))
mus_uni$duration_ms <- as.double(as.character(mus_uni$duration_ms))
```

Asimismo, realizamos este procedimiento para las varibales que son de tipo character para volver a convertirlas, en el mismo tipo de dato, que es cahracter. Y luego se realiza un summary para verificar que no haya quedado alguna variable NA, luego de realizar las conversiones de tipo de datos.

```{r transformar variables chars}
mus_uni$track_id <- as.character(mus_uni$track_id)
mus_uni$track_name <- as.character(mus_uni$track_name)
mus_uni$artist_name <- as.character(mus_uni$artist_name)
summary(mus_uni)

```

## Muestreo Aleatorio

El siguiente paso es uno sumamente importante, el cual consiste en generar una muestra aleatoria apartir de la base general de mus_uni. Esto se hace debido a que la cantidad de datos es suamamente grande, por ende, es mejor utilizar una muestra que sea más pequeña, pero que sea representativa de la base de datos original. Para esto se calcularan las media y desviaciones estándar de cada variable y luego se hará lo mismo para la muestra. De esta manera buscaremos que los valores para las medias y desviaciones, sean lo mas similares posibles entre las dos muestras.

```{r}
set.seed(1000)
mus_muest <- mus_uni[sample(nrow(mus_uni), 10000),]
summary(mus_muest)
```

A continuación se procede a calcular los valores de las medias y desviaciones estandares, tanto para la base de datos original, como para la muestra. De esta manera podremos saber si el tamaño es correcto para nuestro análisis.

```{r}
mus_num <- mus_uni %>% 
  select(c("danceability", "energy", "key", "loudness", "mode", "speechiness", "acousticness", "instrumentalness", "liveness", "valence", "tempo"))
"Media Base Original"
apply(mus_num, 2,mean)

"Desviación Estándar Base Original"
apply(mus_num, 2, sd)
```

Ahora vienen la media y desviación estándar de la muestra.

```{r}
mus_num2 <- mus_muest %>% 
  select(c("danceability", "energy", "key", "loudness", "mode", "speechiness", "acousticness", "instrumentalness", "liveness", "valence", "tempo"))
"Media Base Original"
apply(mus_num2, 2,mean)

"Desviación Estándar Base Original"
apply(mus_num2, 2, sd)
```
Luego por medio de inspección visual podemos apreciar que la muestra es representativa, ya que las variaciones existentes entre las medias y desviaciones estándares, son mínimas. Por ende procederemos a realizar los cálculos con la muestra creada.

## Escalamiento de los Datos

Luego en este punto se procede a escalar la data. El sentido de esto es poder trabajar en medidas que sean mas pequeñas y que por ende, permitan que los analisis sean mas sencillos. Es por esto que lo primero que se debe hacer es separar las variables que sean numericas de las caracteres, en la base de datos de la muestra.

```{r}
songs_char <- mus_muest %>% 
  select(c("artist_name", "track_id", "track_name"))
songs_num <- mus_muest %>%
  select(c("key", "danceability", "energy", "loudness", "mode", "speechiness", "acousticness", "instrumentalness", "liveness", "valence", "tempo", "duration_ms"))
```

Luego, se procede a realizar el escalado de la data para los posteriores analisis de clusters. Esto es el escalamiento de las variables numericas, evidentemente.

```{r}
mus_scale <- data.frame(sapply(songs_num, scale))
```

A continuación se procede a revisar que la data escalada no tenga valores que sean NA. En caso de existir, estos son retirados.

```{r}
summary(mus_scale)
```

# Procesamiento de los Datos

## Análisis de Clusters

### Kmeans

Como ya tenemos nuestra data muestral, escalada y sin presencia de datos NA, podemos comenzar con el proceso de análisis de clusters. Comenzaremos con K means, buscando que hayan 10 clusters, para luego ir variando estos resultados acorde a lo que se vaya descubriendo. En este paso se clona la variable de mus_scale, para que no modifique a la original, y de esta manera si en el futuro necesitamos usar la original, no tengamos algun inconveniente. Por eso, se crea la variable mus_scale10.

```{r}
kmeans10 <- kmeans(mus_scale, centers = 10)
mus_scale10 <- mus_scale
mus_scale10$clus <- kmeans10$cluster %>% as.factor()
ggplot(mus_scale10, aes(liveness, energy, color=clus)) +
  geom_point(alpha=0.5, show.legend = T) +
  theme_bw()
ggplot(mus_scale10, aes(acousticness, speechiness, color=clus)) +
  geom_point(alpha=0.5, show.legend = T) +
  theme_bw()
ggplot(mus_scale10, aes(loudness, instrumentalness, color=clus)) +
  geom_point(alpha=0.5, show.legend = T) +
  theme_bw()
```

Podemos ver por medio de inspección visual que este valor para K no es ideal, ya que los clusters estan sumamente desordenados. Además debemos cambiar este valor, porque nosotros queremos clusters en donde existan canciones similares, y podemos apreciar a simple vista que todo esta muy mezclado. Por ende, procedemos a realizar el calculo del coeficiente de silueta, para de esta manera lograr encontrar el valor ideal que debiese tener K.

### Coeficiente de Silueta

```{r coeficiente silueta}
coefSil=numeric(30)
for (k in 2:30){
  modelo <- kmeans(mus_scale, centers = k)
  temp <- silhouette(modelo$cluster,dist(mus_scale))
  coefSil[k] <- mean(temp[,3])
}
tempDF=data.frame(CS=coefSil,K=c(1:30))
ggplot(tempDF, aes(x=K, y=CS)) + 
  geom_line() +
  scale_x_continuous(breaks=c(1:30))
```

De este modelo podemos ver el valor de K ideal sería de 2, pero esto seria erróneo y contraproducente hacerlo, ya que si para el valor de 10, los datos estaban desordenados, para el valor de 2 será aún mayor el desorden, ya que los datos estarían en dos clusters. Se procede a mostrar esto gráficamente. Para un valor de K=2. Finalmente, se toma como una idea, el intentar aplicar metodos de clusterización sobre el primer cluster generado al utilizar k = 2. En este paso se clona la variable de mus_scale, para que no modifique a la original, y de esta manera si en el futuro necesitamos usar la original, no tengamos algun inconveniente. Por eso, se crea la variable mus_scale2.

```{r kmeans 2 clusters}
kmeans2 = kmeans(mus_scale, centers = 2)
mus_scale2 <- mus_scale
mus_scale2$clus <- kmeans2$cluster %>% as.factor()
ggplot(mus_scale2, aes(liveness, energy, color=clus)) +
  geom_point(alpha=0.5, show.legend = T) +
  theme_bw()
ggplot(mus_scale2, aes(acousticness, speechiness, color=clus)) +
  geom_point(alpha=0.5, show.legend = T) +
  theme_bw()
ggplot(mus_scale2, aes(loudness, instrumentalness, color=clus)) +
  geom_point(alpha=0.5, show.legend = T) +
  theme_bw()
```

Al observar esto, se procede a analizar los datos pertenecientes al primer cluster generado por el modelo al primer cluster. 

```{r}
clu1_k2 <- mus_scale2 %>% filter(mus_scale2$clus == 1)
summary(clu1_k2)
```

### Cluster Probabilístico

Dados los problemas presentados al intentar encontrar un valor para K ideal, se procede a analizar por medio de cluster probabilísticos al primer cluster que se genero al usar el valor de k = 2. Ya que de esta manera podremos lograr encontrar cluster que sigan una distribución prababilistica.
Para esto el modelo intenta optimizar los valores de mu y k, para las conformación de estos clusters. (Según lo visto en clases) Además, este se realizará sobre el primer cluster de K=2.

Para esto se utilizará el modelo de kmeans GMM (Gaussiano Multivariado), ya que tiene un parecido con el kmeans. 

```{r}
kmeansGMM <- Mclust(clu1_k2)
plot(kmeansGMM, what = 'classification')
summary(kmeansGMM)
```

Al realizar la funcion summary sobre nuestra variable que contiene kmeansGMM, podemos apreciar que lamentablemente no se logra obtener información adicional, respecto de que valor sería el ideal para k, y de esta manera generar los clusters necesarios.

## Proceso Invertido de Clusterización

Una manera que se puede intentar obtener el numero ideal de k, es por medio de empezar realizando primero cluster probabilisticos, y luego cuando se tenga esta distribución se procede a ralizar kmeans. De esta manera se podría tener quizás un mayor orden, y de esta manera determinar cual es el k ideal para poder generar los clusters.

El primer paso consiste en generar el grafico de acuerdo al modelo GMM para la data numerica que se encuentra escalada, de esta manera intentaremos identificar si es que surge algun posible valolr para K.

```{r}
gmm <- Mclust(mus_scale)
plot(gmm, what = 'classification')
summary(gmm)
```

Este gráfico si permite diferenciar a una escala muy baja que existe una cierta tendencia de clusters, pero desafortunadamente aún no es posible distinguir algún valor para poder asignarle a k, ya que al aplicar la función de resumen, no es posible aún identificar este valor que nos encontramos buscando.

Lo siguiente que se debe hacer es juntar la muestra aleatoria de los datos junto con el cluster al que esta asignado cada canción, para de esta manera poder identificar a cada una. 

```{r}
mus_muest_clu <- NULL
cl <- gmm$classification
mus_muest_clu <- cbind(mus_muest, cl)
```

A continuación, se procede con elegir la canción de interés,la cual es una de Yandel, que se llama Explicale.

```{r selección canción}
cancion <- mus_muest_clu[123,]
```

Aqui procedemos a identifcar en que cluster se encuentra esta canción. Y podemos ver que esta en el cluster número 7

```{r}
cluster <- mus_muest_clu %>% filter(mus_muest_clu$cl == cancion$cl)
summary(cluster)
```

A continuación, se procede a calcular el valor del coeficiente de siluerta para estos clusters. Y podemos ver de manera gráfica que el k que quizás podría servir es igual a 5.

```{r}
clu_num <- cluster %>% select(c("key", "danceability", "energy", "loudness", "mode", "speechiness", "acousticness", "instrumentalness", "liveness", "valence", "tempo", "duration_ms"))
coefSil=numeric(30)
for (k in 2:30){
  modelo <- kmeans(clu_num, centers = k)
  temp <- silhouette(modelo$cluster,dist(clu_num))
  coefSil[k] <- mean(temp[,3])
}
tempDF=data.frame(CS=coefSil,K=c(1:30))
ggplot(tempDF, aes(x=K, y=CS)) + 
  geom_line() +
  scale_x_continuous(breaks=c(1:30))
```

Luego de lo anterior, se realiza el grafico del codo, para poder visualizar y validar en que punto se debería fijar el k. Para esto se observo en que lugar se produce el quiebre. El cual de manera visual se ve que es en el 5.

```{r}
kmeansSumaCuadradoscluster <- numeric(30)
for(k in 1:30){
  modelo <- kmeans(clu_num, centers = k)
  kmeansSumaCuadradoscluster[k] <- modelo$tot.withinss
}
plot(kmeansSumaCuadradoscluster)
```

De esta manera se reafirma la posición de realizar el análisis de kmeans, con un k = 5. Para la canción que se escogió más arriba. Por esto se hace el modelo, y despues se esto se procede con el momento de filtrar las canciones acorde a su duracion. 
```{r}
modeloKmeansCluster <- kmeans(clu_num, centers = 5)
cluster$cl <- NULL
cluster$cl<- modeloKmeansCluster$cluster
cancion2 <- cluster %>% filter(cluster$track_id == cancion$track_id)
k_cluf <- cluster %>% filter(cluster$cl == cancion2$cl)
```

Es importante señalar que un requisito de este trabajo es que la playlist tenga una duración de 3 horas. Por ende, se tomo la decisión de no transformar el tiempo a minutos, y por ende, se trabajará unicamente en milisegundos. Para esto debemos recordar que 1 minuto equivale a 60.000 milisegundos. Por ende al momento de calcular la duración de la playlist, nos preocuparemos que el tiempo de esta sea menor o igual a 10.800.000 milisegundos.

Para esto lo que se realiza, es agregar la cancion que tenemos como referencia a nuestra playlist, además de su duración. De esta manera programamos un contador que empiece en la duración de la cancion que escogimos. Luego por medio de un ciclo while se procede a agregar canciones de manera random a la playlist. En caso que esta ya esta agregada no se agregara denuevo, esto por medio de un condicional if.

Luego se unen las diferentes filas de esta playlist, para luego ser presentadas al usuario interesado.

```{r playlist}
cancion2 <- cluster %>% filter(cluster$track_id == cancion$track_id)
Playlist <- NULL
c_time <- cancion2$duration_ms
tiempoMax <- 10800000
Playlist <- rbind.data.frame(Playlist, cancion)
while (c_time <= tiempoMax) {
  
  cancionElegida <- k_cluf[sample(nrow(k_cluf), 1),]
  
  if(any(Playlist$track_id == cancionElegida$track_id)){
    next
  }
  
  Playlist <- rbind.data.frame(Playlist, cancionElegida)
  
  c_time = c_time + as.numeric(cancionElegida$duration_ms) 
}
```

## Resultado

Finalmente el último paso, es mostrar las canciones para el usuario. Es importante señalar que para que sea amigable, se mostraran los nombres de las canciones y su artista.

```{r print playlist}
PlaylistPrint <- Playlist %>% select(c("track_name", "artist_name"))
head(as.matrix.data.frame(PlaylistPrint), nrow(PlaylistPrint))
print(c_time)
```
Adicionalmente, se imprime la duracion de esta excede en una muy pequeña parte, de hecho para ser exactos en 18 segundos. Por ende, concluimos queeste es un resultado aceptable.

De esta manera obtenemos la playlist asociada a nuestro proyecto.